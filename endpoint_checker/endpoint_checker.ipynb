{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f33af68-507b-4831-9186-46067200bb87",
   "metadata": {},
   "source": [
    "### Check an endpoint\n",
    "This notebook aims to run the pipeline on a given endpoint to check to see if it will be successful. This includes collecting, pipeline and datset stages. It aims to also highlight useful information as a summary as to whether the endpoint would be successful on our platform. it will download all relevant data to do this and hence might be disk intensive. You'll need to provide the following information:\n",
    "- collection - this is the collection that the dataset belongs to, this can be extracted from the specification but for this notebook we ask to provide it incase you want to test the pipeline on something which isn't being included in the main site right now\n",
    "- dataset - this is the dataset that the endpoint is meant to provide data for, technically this can be multiple datasets but this this use case it should just be one. It is also the name of the pipeline that is ran on the individual resources that are downloaded from the endpoint. the terms dataset/pipline are often the same\n",
    "- organisation - the organisation identifier to be used for the endpoint\n",
    "- endpoint url - the actual url needed for the endpoint\n",
    "- plugin - often we use plugins to download the data this is only needed for specific endpoints\n",
    "\n",
    "If you are seeing errors regarding digital-land, then try \n",
    "\n",
    "`pip install -e git+https://github.com/digital-land/pipeline.git#egg=digital-land`\n",
    "\n",
    "And restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9959de-69a3-4199-86ad-ecf5fa3150c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from functions import run_endpoint_workflow\n",
    "from sqlite_query_functions import DatasetSqlite\n",
    "from convert_functions import convert_resource\n",
    "from digital_land.collection import Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3f438-0b45-485e-b1f2-6e9b4bb01729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#southwark convservation area endpoint\n",
    "# collection_name = 'conservation-area-collection'\n",
    "# dataset = 'conservation-area'\n",
    "# organisation = 'local-authority-eng:SWK'\n",
    "# endpoint_url = 'https://www.southwark.gov.uk/assets/attach/194104/Conservation-Areas.gpkg'\n",
    "# plugin = None\n",
    "# additional_column_mappings=None\n",
    "# additional_concats=None\n",
    "\n",
    "# doncaster tpos\n",
    "# collection_name = 'tree-preservation-order-collection'\n",
    "# dataset = 'tree-preservation-zone'\n",
    "# organisation = 'local-authority-eng:DNC'\n",
    "# endpoint_url='https://maps.doncaster.gov.uk/server/rest/services/Planning/TPO_Map/MapServer/1'\n",
    "# plugin = 'arcgis'\n",
    "# additional_column_mappings=None\n",
    "# additional_concats=None\n",
    "\n",
    "# \n",
    "collection_name = 'listed-building-collection'\n",
    "dataset = 'listed-building-outline'\n",
    "organisation = 'local-authority-eng:CAM'\n",
    "endpoint_url = 'https://opendata.camden.gov.uk/api/views/uu3n-zgbj/rows.csv?accessType=DOWNLOAD'\n",
    "plugin = None\n",
    "\n",
    "additional_column_mappings=None\n",
    "additional_concats=None\n",
    "\n",
    "# generic data_dir setting\n",
    "data_dir = '../data/endpoint_checker'\n",
    "\n",
    "# example playing with additional confiigs\n",
    "# additional_concats = [{\n",
    "#     'dataset':'tree-preservation-zone',\n",
    "#     'endpoint':'de1eb90a8b037292ef8ae14bfabd1184847ef99b7c6296bb6e75379e6c1f9572',\n",
    "#     'resource':'e6b0ccaf9b50a7f57a543484fd291dbe43f52a6231b681c3a7cc5e35a6aba254',\n",
    "#     'field':'reference',\n",
    "#     'fields':'REFVAL;LABEL',\n",
    "#     'separator':'/'\n",
    "# }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38144f-1c71-4283-840e-bddffd3ea776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_endpoint_workflow(\n",
    "    collection_name,\n",
    "    dataset,\n",
    "    organisation,\n",
    "    endpoint_url,\n",
    "    plugin,\n",
    "    data_dir,\n",
    "    additional_col_mappings=additional_column_mappings,\n",
    "    additional_concats=additional_concats\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4ef1f-9456-4e1f-b654-cbb56d5e3cf2",
   "metadata": {},
   "source": [
    "#### Collection log summaries\n",
    "\n",
    "We need to establish if a resource was downloaded from the endpoint and whether there were any issues during the collection process. Examine the output of the below. There should be one log for the attempt made at downloading from the endpoint. If status code is 200 then the resource was downloaded successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad17395f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection = Collection(os.path.join(data_dir,'collection'))\n",
    "collection.load(directory=os.path.join(data_dir,'collection'))\n",
    "\n",
    "collection.resource.records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62286dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logs = collection.log.entries\n",
    "logs = pd.DataFrame.from_records(logs)\n",
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f842a08",
   "metadata": {},
   "source": [
    "### Check unnassigned entiities\n",
    "This process automatically aims to detect and assign entity numbers where entries are currently unnassigned. Examine the list below to see what (if any) entities have been assigned. if you were to include these in an actual pipeline you would need to update the configuration lookup.csv with these values. It's worth checking they are sensible before this happens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unassigned_entries = pd.read_csv(os.path.join(data_dir,'var','cache','unassigned-entries.csv'))\n",
    "if len(unassigned_entries) == 0:\n",
    "    print('no additional entity numbers where required')\n",
    "else:\n",
    "    print(unassigned_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313b0fa-151d-4c52-bb46-f6e733d2369e",
   "metadata": {},
   "source": [
    "#### Check logs collated from the pipeline process\n",
    "We need to read the logs and examine to see if the data points were all read in correctly. This uses the sqlite database to do so with some custom queries. You could directly examine the csvs if the pipeline fails.\n",
    "\n",
    "First, check the column mappings to see what columns the pipeline automatically mapped. Tf this is empty or missing values, then it's likely to be the reason data isn't appearing at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c01973",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_db = DatasetSqlite(os.path.join(data_dir,'dataset',f'{dataset}.sqlite3'))\n",
    "results = dataset_db.get_column_mappings()\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c9e67-e108-424e-a5cb-6aab2cbde912",
   "metadata": {},
   "source": [
    "examine the issues logs, we'll look at the types of errors being raised and list all of them. This could be improved in the future by examining the severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_db = DatasetSqlite(os.path.join(data_dir,'dataset',f'{dataset}.sqlite3'))\n",
    "results = dataset_db.get_issues_by_type()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2555d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset_db.get_issues()\n",
    "\n",
    "\n",
    "results.loc[(results.issue_type != 'default-value') ]\n",
    "\n",
    "# duff_geom = results.loc[(results.issue_type == 'invalid geometry')]\n",
    "\n",
    "duff_geom = results.loc[(results.issue_type == 'Unexpected geom type')]\n",
    "\n",
    "cols = duff_geom[[\"line_number\", \"value\"]]\n",
    "\n",
    "print (cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f612d0-ca02-4080-b6f6-847f2b1ea22c",
   "metadata": {},
   "source": [
    "#### Final dataset comparison against the sqlite database\n",
    "\n",
    "Below are two tables which show the difference betwen what was provided to us and what is ucrrently in the entity table. It is important to bear in mind that we assign entities automaticallyis process, the table above shows what we have added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7b0a5-3d38-4389-bf3c-413ecae98a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_db = DatasetSqlite(os.path.join(data_dir,'dataset',f'{dataset}.sqlite3'))\n",
    "results = dataset_db.get_entities()\n",
    "results.head(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in raw resources\n",
    "collection = Collection(os.path.join(data_dir,'collection'))\n",
    "collection.load(directory=os.path.join(data_dir,'collection'))\n",
    "resources = collection.resource.entries\n",
    "resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b20c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently this just reads in the raw resource but in  the future this should check for \n",
    "# converted resource first\n",
    "resource = resources[0]['resource']\n",
    "resource_path = os.path.join(data_dir,'collection','resource',resource)\n",
    "try:\n",
    "    raw_resource = pd.read_csv(resource_path)\n",
    "except (UnicodeDecodeError,TypeError,pd.errors.ParserError):\n",
    "    converted_resource_dir = os.path.join(data_dir,'var','converted_resources')\n",
    "    converted_resource_path = os.path.join(converted_resource_dir,f'{resource}.csv') \n",
    "    if not os.path.exists(converted_resource_path):\n",
    "        convert_resource(resource,resource_path,converted_resource_dir,dataset)\n",
    "    raw_resource = pd.read_csv(converted_resource_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da5690",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_resource"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
